{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeae0d30-cd75-44d4-8bbe-d48dd8d66fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict  # Necesaria para validación cruzada\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91981610-61c9-4e23-b22f-19303cc7eba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85759a11-a536-4f37-862d-de3d52bee043",
   "metadata": {},
   "source": [
    "### En primer lugar se prueban los modelos más comúnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744ee771-852a-47a0-9937-46c741a9ffc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_data= pd.read_csv(\"Data/train_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84387b7c-6c29-4ff8-a55e-6b77cd0ff6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_23952\\3298826243.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "C:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_23952\\3298826243.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "C:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_23952\\3298826243.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n",
      "C:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_23952\\3298826243.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model RMSLE_train RMSLE_test  R2_train    R2_test\n",
      "0           Decision Tree        0.00       0.20      1.00       0.71\n",
      "1           Random Forest        0.06       0.14      0.98       0.89\n",
      "2       Gradient Boosting        0.08       0.14      0.97       0.90\n",
      "3  Support Vector Machine        0.40       0.42     -0.05      -0.06\n",
      "4       Linear Regression    3.40e-01   2.66e+00  8.90e-01  -2.49e+21\n",
      "Escalador guardado como Data/minmax_scaler.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar\\AppData\\Local\\Temp\\ipykernel_23952\\3298826243.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append({\n"
     ]
    }
   ],
   "source": [
    "# Separar las características y la variable objetivo\n",
    "X = cleaned_data.drop(columns=['SalePrice'])\n",
    "y = cleaned_data['SalePrice']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de los datos usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Modelos a probar\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'Support Vector Machine': SVR()\n",
    "}\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados\n",
    "results = pd.DataFrame(columns=['Model', 'RMSLE_train', 'R2_train', 'RMSLE_test', 'R2_test'])\n",
    "\n",
    "# Función para calcular RMSLE asegurando que los valores sean positivos\n",
    "def rmsle(y_true, y_pred):\n",
    "    # Asegurarse de que todos los valores sean mayores o iguales a 0\n",
    "    y_true = np.where(y_true < 0, 0, y_true)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predicciones en el conjunto de entrenamiento\n",
    "    train_predictions = model.predict(X_train_scaled)\n",
    "    rmsle_train = rmsle(y_train, train_predictions)\n",
    "    r2_train = r2_score(y_train, train_predictions)\n",
    "    \n",
    "    # Predicciones en el conjunto de validación\n",
    "    test_predictions = model.predict(X_valid_scaled)\n",
    "    rmsle_test = rmsle(y_valid, test_predictions)\n",
    "    r2_test = r2_score(y_valid, test_predictions)\n",
    "    \n",
    "    # Almacenar los resultados\n",
    "    results = results.append({\n",
    "        'Model': model_name,\n",
    "        'RMSLE_train': rmsle_train,\n",
    "        'R2_train': r2_train,\n",
    "        'RMSLE_test': rmsle_test,\n",
    "        'R2_test': r2_test\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Reorganizar las columnas\n",
    "results = results[['Model', 'RMSLE_train', 'RMSLE_test', 'R2_train', 'R2_test']]\n",
    "\n",
    "# Redondear a dos decimales para RMSLE y R2\n",
    "results[['RMSLE_train', 'RMSLE_test']] = results[['RMSLE_train', 'RMSLE_test']].round(2)\n",
    "results[['R2_train', 'R2_test']] = results[['R2_train', 'R2_test']].round(2)\n",
    "\n",
    "# Formatear RMSLE y R2: \n",
    "def format_rmsle(value, model_name):\n",
    "    if model_name == 'Linear Regression':\n",
    "        return f\"{value:.2e}\"  # Notación científica\n",
    "    else:\n",
    "        return f\"{value:,.2f}\"  # Formato legible\n",
    "\n",
    "# Aplicar formato a RMSLE\n",
    "results['RMSLE_train'] = [format_rmsle(value, model) for value, model in zip(results['RMSLE_train'], results['Model'])]\n",
    "results['RMSLE_test'] = [format_rmsle(value, model) for value, model in zip(results['RMSLE_test'], results['Model'])]\n",
    "\n",
    "# Formatear R2: notación científica solo para Linear Regression\n",
    "def format_r2(value, model_name):\n",
    "    if model_name == 'Linear Regression':\n",
    "        return f\"{value:.2e}\"  # Notación científica\n",
    "    else:\n",
    "        return f\"{value:.2f}\"  # Formato legible\n",
    "\n",
    "results['R2_train'] = [format_r2(value, model) for value, model in zip(results['R2_train'], results['Model'])]\n",
    "results['R2_test'] = [format_r2(value, model) for value, model in zip(results['R2_test'], results['Model'])]\n",
    "\n",
    "# Mostrar los resultados ordenados por RMSLE_train y RMSLE_test\n",
    "results = results.sort_values(by=['RMSLE_train', 'RMSLE_test']).reset_index(drop=True)\n",
    "print(results)\n",
    "\n",
    "# Guardar el escalador en la subcarpeta Data\n",
    "scaler_filename = 'Data/minmax_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "print(f\"Escalador guardado como {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169b075-7758-40e2-a34a-9f137b0ba1e6",
   "metadata": {},
   "source": [
    "Se usan dos métricas RMSLE que es la que se usará como referencia en el reto y $R^2$ que se medirán tanto en train como en test, con el objetivo de valorar el sobreajuste.\n",
    "Se observan métricas prometedoras en los modelos más robustos (Gradient Boosting y Random Forest). Los modelos que no funcionan bien con relaciones no lineales tienen un desempeño muy pobre. Todo ello sugiere que las variables se relacionan entre si de forma compleja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8ebde-8f1c-46dd-a444-d655594f9ed5",
   "metadata": {},
   "source": [
    " ### Modelos de ensamblado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9852b3-8a43-45d8-99b7-c141d563c4d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2478\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166, number of used features: 70\n",
      "[LightGBM] [Info] Start training from score 180976.861063\n",
      "               Model  RMSLE_train  R2_train  RMSLE_test  R2_test\n",
      "0  Gradient Boosting         0.08      0.97        0.13     0.90\n",
      "1           LightGBM         0.05      0.98        0.14     0.89\n",
      "2            XGBoost         0.01      1.00        0.14     0.88\n",
      "3           AdaBoost         0.19      0.88        0.22     0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 217, in _count_physical_cores\n",
      "    raise ValueError(\n"
     ]
    }
   ],
   "source": [
    "# Separar las características y la variable objetivo\n",
    "X = cleaned_data.drop(columns=['SalePrice'])\n",
    "y = cleaned_data['SalePrice']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de los datos usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# Modelos a probar\n",
    "models = {\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100),\n",
    "    \n",
    "}\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados\n",
    "results = pd.DataFrame(columns=['Model', 'RMSLE_train', 'R2_train', 'RMSLE_test', 'R2_test'])\n",
    "\n",
    "# Función para calcular RMSLE asegurando que los valores sean no negativos\n",
    "def rmsle(y_true, y_pred):\n",
    "    # Asegurar que todos los valores sean mayores o iguales a 0\n",
    "    y_true = np.where(y_true < 0, 0, y_true)\n",
    "    y_pred = np.where(y_pred < 0, 0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predicciones en el conjunto de entrenamiento\n",
    "    train_predictions = model.predict(X_train_scaled)\n",
    "    rmsle_train = rmsle(y_train, train_predictions)\n",
    "    r2_train = r2_score(y_train, train_predictions)\n",
    "\n",
    "    # Predicciones en el conjunto de validación\n",
    "    test_predictions = model.predict(X_valid_scaled)\n",
    "    rmsle_test = rmsle(y_valid, test_predictions)\n",
    "    r2_test = r2_score(y_valid, test_predictions)\n",
    "\n",
    "    # Almacenar los resultados\n",
    "    results = pd.concat([results, pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'RMSLE_train': [rmsle_train],\n",
    "        'RMSLE_test': [rmsle_test],\n",
    "        'R2_train': [r2_train],\n",
    "        'R2_test': [r2_test]\n",
    "    })], ignore_index=True)\n",
    "\n",
    "# Mostrar los resultados ordenados por RMSLE de prueba\n",
    "results = results.sort_values(by='RMSLE_test').reset_index(drop=True)\n",
    "\n",
    "# Mostrar resultados con dos decimales\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d324e-4a4f-434d-a1ea-3a4965b69298",
   "metadata": {},
   "source": [
    "Dado los buenos resultados con Gradient Boosting se prueban otros modelos de la misma famila para ver si alguno lo mejora, siendo la diferencia prácticamente nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c10e6-d7e9-49bb-a315-7f230ede63d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b414731f-d911-49ea-8597-1595149d571b",
   "metadata": {},
   "source": [
    "### CatBosst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2ed6f0-b545-434e-8339-cfa06340f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"Data/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aab8ecf-48cd-44bb-8d21-a57bec7c1ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 72162.6819362\ttotal: 300ms\tremaining: 5m\n",
      "100:\tlearn: 16906.5345257\ttotal: 11.6s\tremaining: 1m 43s\n",
      "200:\tlearn: 12491.7206160\ttotal: 23.4s\tremaining: 1m 32s\n",
      "300:\tlearn: 10194.6269635\ttotal: 34.9s\tremaining: 1m 20s\n",
      "400:\tlearn: 8539.2389198\ttotal: 46.7s\tremaining: 1m 9s\n",
      "500:\tlearn: 7423.0275130\ttotal: 57.9s\tremaining: 57.7s\n",
      "600:\tlearn: 6521.4174597\ttotal: 1m 9s\tremaining: 46.1s\n",
      "700:\tlearn: 5611.5770354\ttotal: 1m 21s\tremaining: 34.6s\n",
      "800:\tlearn: 4836.5708256\ttotal: 1m 32s\tremaining: 23s\n",
      "900:\tlearn: 4229.1078623\ttotal: 1m 44s\tremaining: 11.4s\n",
      "999:\tlearn: 3845.4652330\ttotal: 1m 55s\tremaining: 0us\n",
      "CatBoost Metrics:\n",
      "Train RMSLE: 0.05, Train R²: 0.99\n",
      "Validation RMSLE: 0.14, Validation R²: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Separar las características y la variable objetivo\n",
    "X = data.drop(columns=['SalePrice'])  # Cambia 'SalePrice' por tu variable objetivo\n",
    "y = data['SalePrice']\n",
    "\n",
    "# Manejar los NaNs en las características categóricas\n",
    "X = X.fillna(\"Desconocido\")\n",
    "\n",
    "# Convertir las características categóricas a tipo string\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "X[categorical_cols] = X[categorical_cols].astype(str)\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identificar las características categóricas\n",
    "categorical_features_indices = [X_train.columns.get_loc(col) for col in categorical_cols]\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, cat_features=categorical_features_indices, verbose=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "train_preds = model.predict(X_train)\n",
    "valid_preds = model.predict(X_valid)\n",
    "\n",
    "# Definir función para RMSLE\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_true = np.maximum(0, y_true)  # Ajuste para valores no negativos\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "train_rmsle = rmsle(y_train, train_preds)\n",
    "valid_rmsle = rmsle(y_valid, valid_preds)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "valid_r2 = r2_score(y_valid, valid_preds)\n",
    "\n",
    "# Resultados\n",
    "print(\"CatBoost Metrics:\")\n",
    "print(f\"Train RMSLE: {train_rmsle:.2f}, Train R²: {train_r2:.2f}\")\n",
    "print(f\"Validation RMSLE: {valid_rmsle:.2f}, Validation R²: {valid_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958950f8-152c-45bc-ab05-a21df0989dbb",
   "metadata": {},
   "source": [
    "También se prueba CatBoost que es un modelo de ensamblaje que tiene la particularidad de admitir datos categóricos. Las métricas en test son ligeramante superiores pero el sobreajuste, que ya era elevado en los modelos numéricos, es inadmisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7da86-9cef-46a2-bc76-ed4517d718ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e678f274-422f-4ce8-aced-d5fd5b4d255d",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20dcfb2f-d650-4f6f-9e87-5482b582b358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_data= pd.read_csv(\"Data/train_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bd94ad-2b19-4cc1-aa51-345109d24251",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2478\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166, number of used features: 70\n",
      "[LightGBM] [Info] Start training from score 180976.861063\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2251\n",
      "[LightGBM] [Info] Number of data points in the train set: 932, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 181194.222103\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2247\n",
      "[LightGBM] [Info] Number of data points in the train set: 933, number of used features: 67\n",
      "[LightGBM] [Info] Start training from score 179821.118971\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2258\n",
      "[LightGBM] [Info] Number of data points in the train set: 933, number of used features: 67\n",
      "[LightGBM] [Info] Start training from score 182562.898178\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2251\n",
      "[LightGBM] [Info] Number of data points in the train set: 933, number of used features: 69\n",
      "[LightGBM] [Info] Start training from score 180411.956056\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2250\n",
      "[LightGBM] [Info] Number of data points in the train set: 933, number of used features: 68\n",
      "[LightGBM] [Info] Start training from score 180894.342980\n",
      "Stacking Model - RMSLE Train: 0.08, R2 Train: 0.93\n",
      "Stacking Model - RMSLE Test: 0.15, R2 Test: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Separar las características y la variable objetivo\n",
    "X = cleaned_data.drop(columns=['SalePrice'])\n",
    "y = cleaned_data['SalePrice']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de los datos usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "base_models = [\n",
    "    ('random_forest', RandomForestRegressor(n_estimators=100)),\n",
    "    ('gradient_boosting', GradientBoostingRegressor(n_estimators=100)),\n",
    "    ('lightgbm', LGBMRegressor(n_estimators=100))\n",
    "]\n",
    "\n",
    "meta_model = XGBRegressor(\n",
    "    n_estimators=100,   \n",
    "    learning_rate=0.1,  \n",
    "    max_depth=3,        \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Definir el modelo de stacking\n",
    "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Entrenar el modelo de stacking\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predicciones en el conjunto de entrenamiento y validación\n",
    "train_predictions = stacking_model.predict(X_train_scaled)\n",
    "test_predictions = stacking_model.predict(X_valid_scaled)\n",
    "\n",
    "# Definir función para RMSLE\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_true = np.maximum(0, y_true)  # Asegurarse de que los valores sean no negativos\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "rmsle_train = rmsle(y_train, train_predictions)\n",
    "r2_train = r2_score(y_train, train_predictions)\n",
    "\n",
    "rmsle_test = rmsle(y_valid, test_predictions)\n",
    "r2_test = r2_score(y_valid, test_predictions)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Stacking Model - RMSLE Train: {rmsle_train:.2f}, R2 Train: {r2_train:.2f}\")\n",
    "print(f\"Stacking Model - RMSLE Test: {rmsle_test:.2f}, R2 Test: {r2_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab5607-1cc5-479b-beba-7bd0e35703c0",
   "metadata": {},
   "source": [
    "Se usa un stacking con los modelos que mejor han funcionado como modelos base y XGBBoost como metamodelo para captar las realciones no lineales de forma eficiente. Los resultados son interesantes porque si bien bajamos ligeramente en test, el sobreajuste es considerablemente menor. La parte negativa es que un stacking es mucho más costoso de optimizar que un modelo simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73516e-e9ea-4f48-96ff-699d05057d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba705b70-8664-452b-933d-39122a771772",
   "metadata": {},
   "source": [
    "### Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee463c4-06b8-43ed-bcd9-12bedc89537d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\oscar\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 39280422912.0000 - mae: 180972.0625 - val_loss: 38041677824.0000 - val_mae: 180543.3906\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 39251369984.0000 - mae: 180893.9219 - val_loss: 37964636160.0000 - val_mae: 180336.3906\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 39047983104.0000 - mae: 180345.9062 - val_loss: 37545947136.0000 - val_mae: 179209.2031\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 38228910080.0000 - mae: 178153.3906 - val_loss: 36159430656.0000 - val_mae: 175427.8906\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 36031025152.0000 - mae: 172051.2656 - val_loss: 32896978944.0000 - val_mae: 166201.3438\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 31506999296.0000 - mae: 158929.9688 - val_loss: 27012403200.0000 - val_mae: 148153.8750\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 24296992768.0000 - mae: 135501.9844 - val_loss: 18710355968.0000 - val_mae: 118435.2891\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 15680709632.0000 - mae: 100072.9922 - val_loss: 10211527680.0000 - val_mae: 78961.5547\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 8489600512.0000 - mae: 62219.1094 - val_loss: 5091176448.0000 - val_mae: 49854.2383\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 5155893248.0000 - mae: 45340.2188 - val_loss: 3551976960.0000 - val_mae: 45417.6992\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4420760576.0000 - mae: 45440.2773 - val_loss: 3319805184.0000 - val_mae: 45623.9961\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4258780160.0000 - mae: 45980.0742 - val_loss: 3191022336.0000 - val_mae: 44814.1953\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4128127744.0000 - mae: 45232.1445 - val_loss: 3071452672.0000 - val_mae: 43903.5977\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 4001162752.0000 - mae: 44007.5391 - val_loss: 2955266560.0000 - val_mae: 42958.3438\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3884768256.0000 - mae: 43044.3906 - val_loss: 2847619584.0000 - val_mae: 42039.7695\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3775088896.0000 - mae: 42825.9023 - val_loss: 2743987968.0000 - val_mae: 41671.1094\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3664828928.0000 - mae: 41639.3359 - val_loss: 2643502080.0000 - val_mae: 40595.7344\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3560231680.0000 - mae: 41203.0156 - val_loss: 2544407808.0000 - val_mae: 39824.5977\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3458684928.0000 - mae: 40292.9141 - val_loss: 2453951744.0000 - val_mae: 39241.8359\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3365382144.0000 - mae: 39712.5469 - val_loss: 2362731520.0000 - val_mae: 38315.2344\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3267234560.0000 - mae: 38934.8711 - val_loss: 2275611392.0000 - val_mae: 37600.8164\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3174902272.0000 - mae: 38330.2969 - val_loss: 2192802816.0000 - val_mae: 36905.5312\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3089709568.0000 - mae: 38153.8633 - val_loss: 2108285312.0000 - val_mae: 36170.0977\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3015452928.0000 - mae: 36579.7148 - val_loss: 2031814912.0000 - val_mae: 35373.2930\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2922671104.0000 - mae: 36197.7891 - val_loss: 1955558784.0000 - val_mae: 34475.3398\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2849561600.0000 - mae: 36045.1211 - val_loss: 1882370432.0000 - val_mae: 33876.0781\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2772638976.0000 - mae: 34401.5898 - val_loss: 1813283584.0000 - val_mae: 32730.5645\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2691311616.0000 - mae: 34560.2852 - val_loss: 1754337280.0000 - val_mae: 32625.4785\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 2623971840.0000 - mae: 34002.2578 - val_loss: 1684397696.0000 - val_mae: 31434.7500\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2557547776.0000 - mae: 33218.1562 - val_loss: 1625172864.0000 - val_mae: 30789.7402\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2491625728.0000 - mae: 32283.5820 - val_loss: 1571198976.0000 - val_mae: 30314.4863\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2430218496.0000 - mae: 32263.1250 - val_loss: 1520216064.0000 - val_mae: 29864.6914\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2378606080.0000 - mae: 31191.1328 - val_loss: 1471178368.0000 - val_mae: 29130.8867\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2316856832.0000 - mae: 31216.2773 - val_loss: 1426738816.0000 - val_mae: 28768.4785\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2265551104.0000 - mae: 30754.8672 - val_loss: 1377876864.0000 - val_mae: 27981.4609\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2215157760.0000 - mae: 29996.9258 - val_loss: 1337392640.0000 - val_mae: 27475.6680\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2171572992.0000 - mae: 29222.8711 - val_loss: 1310154752.0000 - val_mae: 27534.3105\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2125625856.0000 - mae: 29415.9766 - val_loss: 1271490048.0000 - val_mae: 26944.9082\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2083910400.0000 - mae: 29025.5605 - val_loss: 1237100800.0000 - val_mae: 26464.2852\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2046534144.0000 - mae: 28214.6328 - val_loss: 1210975360.0000 - val_mae: 26191.6660\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2008358016.0000 - mae: 27945.3789 - val_loss: 1184928512.0000 - val_mae: 25883.6562\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1974318848.0000 - mae: 27785.2598 - val_loss: 1160155008.0000 - val_mae: 25609.4082\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1942382080.0000 - mae: 27327.8145 - val_loss: 1135995136.0000 - val_mae: 25247.0449\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1916166272.0000 - mae: 27295.5918 - val_loss: 1113710976.0000 - val_mae: 24948.4180\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1885514112.0000 - mae: 26988.5762 - val_loss: 1099325568.0000 - val_mae: 24943.5840\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1863671168.0000 - mae: 27044.2852 - val_loss: 1076819968.0000 - val_mae: 24453.5547\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1832079360.0000 - mae: 26281.9395 - val_loss: 1065996352.0000 - val_mae: 24535.9004\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1807777408.0000 - mae: 26261.8984 - val_loss: 1053860992.0000 - val_mae: 24440.6445\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1788880384.0000 - mae: 26371.9473 - val_loss: 1038094144.0000 - val_mae: 24199.3867\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1766364800.0000 - mae: 25784.9180 - val_loss: 1033319552.0000 - val_mae: 24306.2363\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1744750848.0000 - mae: 26086.9434 - val_loss: 1014692416.0000 - val_mae: 23962.8320\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1726656384.0000 - mae: 25447.2363 - val_loss: 1012750720.0000 - val_mae: 24115.3887\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1707161856.0000 - mae: 25705.9043 - val_loss: 1000580992.0000 - val_mae: 23947.1836\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1689656448.0000 - mae: 25461.5137 - val_loss: 996731392.0000 - val_mae: 24001.4395\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1673174528.0000 - mae: 25636.4082 - val_loss: 976978496.0000 - val_mae: 23633.8965\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1657831808.0000 - mae: 25023.0195 - val_loss: 969270016.0000 - val_mae: 23578.4395\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1641289472.0000 - mae: 25262.2480 - val_loss: 963136896.0000 - val_mae: 23565.8906\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1624004736.0000 - mae: 24982.4824 - val_loss: 958732416.0000 - val_mae: 23561.5137\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1612050176.0000 - mae: 25004.2793 - val_loss: 948128192.0000 - val_mae: 23435.1641\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1592316928.0000 - mae: 24835.4824 - val_loss: 940760640.0000 - val_mae: 23362.2012\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1579809408.0000 - mae: 24779.3496 - val_loss: 931875840.0000 - val_mae: 23253.5527\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1565147648.0000 - mae: 24456.8770 - val_loss: 924478080.0000 - val_mae: 23174.2422\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1552891264.0000 - mae: 24586.7246 - val_loss: 928445376.0000 - val_mae: 23346.8965\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1537293696.0000 - mae: 24510.7285 - val_loss: 908234304.0000 - val_mae: 23013.2148\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1528928512.0000 - mae: 24430.3535 - val_loss: 901503424.0000 - val_mae: 22962.7402\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1515345664.0000 - mae: 24403.0488 - val_loss: 896019392.0000 - val_mae: 22932.6836\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1499272448.0000 - mae: 23957.8711 - val_loss: 900738368.0000 - val_mae: 23056.7930\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1500252416.0000 - mae: 24440.7891 - val_loss: 880681280.0000 - val_mae: 22718.2578\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1477427968.0000 - mae: 23943.1387 - val_loss: 879329984.0000 - val_mae: 22795.0352\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1463065344.0000 - mae: 24033.4531 - val_loss: 873515584.0000 - val_mae: 22743.7246\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1453012864.0000 - mae: 23674.0254 - val_loss: 867551936.0000 - val_mae: 22670.8379\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1458489216.0000 - mae: 24123.6934 - val_loss: 859444224.0000 - val_mae: 22572.7637\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1428644608.0000 - mae: 23721.3789 - val_loss: 861191936.0000 - val_mae: 22652.6504\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1420352256.0000 - mae: 23412.3301 - val_loss: 869547520.0000 - val_mae: 22807.6270\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1407289856.0000 - mae: 23662.3926 - val_loss: 848887360.0000 - val_mae: 22499.3398\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1394451456.0000 - mae: 23190.4570 - val_loss: 853309056.0000 - val_mae: 22596.8691\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1397497344.0000 - mae: 23782.8555 - val_loss: 833836160.0000 - val_mae: 22285.4844\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1375369856.0000 - mae: 23140.9922 - val_loss: 833718336.0000 - val_mae: 22317.0195\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1362675712.0000 - mae: 23046.9922 - val_loss: 828854720.0000 - val_mae: 22267.1934\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1352230144.0000 - mae: 23161.0684 - val_loss: 826780800.0000 - val_mae: 22255.7598\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1360883968.0000 - mae: 23080.5488 - val_loss: 832515264.0000 - val_mae: 22396.6074\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1334160256.0000 - mae: 22814.4668 - val_loss: 815123456.0000 - val_mae: 22101.8613\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1325892096.0000 - mae: 22722.9492 - val_loss: 806389824.0000 - val_mae: 21955.4648\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1312315136.0000 - mae: 22543.4668 - val_loss: 811593024.0000 - val_mae: 22112.4473\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1305467136.0000 - mae: 22773.8945 - val_loss: 804839616.0000 - val_mae: 22021.8477\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1300429056.0000 - mae: 22470.9648 - val_loss: 796274432.0000 - val_mae: 21879.3066\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1293026688.0000 - mae: 22479.0762 - val_loss: 805887808.0000 - val_mae: 22098.1484\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1279816960.0000 - mae: 22244.7070 - val_loss: 796880640.0000 - val_mae: 21952.6758\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1272523008.0000 - mae: 22281.0254 - val_loss: 787341760.0000 - val_mae: 21807.1973\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1263114752.0000 - mae: 22256.1094 - val_loss: 787314688.0000 - val_mae: 21834.2773\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1254767616.0000 - mae: 22003.3984 - val_loss: 784132480.0000 - val_mae: 21801.9609\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1255440896.0000 - mae: 21966.2148 - val_loss: 792022976.0000 - val_mae: 21948.9336\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1251006080.0000 - mae: 22255.7148 - val_loss: 774965248.0000 - val_mae: 21675.0098\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1237781504.0000 - mae: 21965.8516 - val_loss: 778281664.0000 - val_mae: 21770.5176\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1224740352.0000 - mae: 21816.7617 - val_loss: 771971392.0000 - val_mae: 21676.7012\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1219312000.0000 - mae: 21749.5996 - val_loss: 777674368.0000 - val_mae: 21795.2422\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1213524992.0000 - mae: 21781.5781 - val_loss: 768624832.0000 - val_mae: 21663.0762\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1207359104.0000 - mae: 21763.0586 - val_loss: 764328320.0000 - val_mae: 21603.3691\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1198243584.0000 - mae: 21687.3398 - val_loss: 760380352.0000 - val_mae: 21561.5000\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1190107136.0000 - mae: 21493.7988 - val_loss: 757041472.0000 - val_mae: 21521.0078\n",
      "37/37 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 2ms/step\n",
      "RMSLE Train: 0.16, R² Train: 0.82\n",
      "RMSLE Test: 0.17, R² Test: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Separar las características y la variable objetivo\n",
    "X = cleaned_data.drop(columns=['SalePrice'])\n",
    "y = cleaned_data['SalePrice']\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y validación\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de los datos\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Capa de entrada\n",
    "    layers.Dense(64, activation='relu'),  # Capa oculta\n",
    "    layers.Dense(32, activation='relu'),  # Capa oculta\n",
    "    layers.Dense(1)  # Capa de salida\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train_scaled, y_train, \n",
    "                    epochs=100,  \n",
    "                    batch_size=32,  \n",
    "                    validation_data=(X_valid_scaled, y_valid),  \n",
    "                    verbose=1)  \n",
    "\n",
    "# Hacer predicciones\n",
    "train_predictions = model.predict(X_train_scaled).flatten()\n",
    "valid_predictions = model.predict(X_valid_scaled).flatten()\n",
    "\n",
    "# Definir función para RMSLE\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_true = np.maximum(0, y_true)  # Asegurarse de que los valores sean no negativos\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Calcular RMSLE y R²\n",
    "rmsle_train = rmsle(y_train, train_predictions)\n",
    "r2_train = r2_score(y_train, train_predictions)\n",
    "\n",
    "rmsle_test = rmsle(y_valid, valid_predictions)\n",
    "r2_test = r2_score(y_valid, valid_predictions)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f'RMSLE Train: {rmsle_train:.2f}, R² Train: {r2_train:.2f}')\n",
    "print(f'RMSLE Test: {rmsle_test:.2f}, R² Test: {r2_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c472929a-6416-49ed-a250-172007a92a95",
   "metadata": {},
   "source": [
    "Al tratarse de un dataset complejo con relaciones no lineales era obligado probar también una red neuronal, de forma similar aunque más acentuada los resultados de test son inferiores a Gradient Boosting pero el sobeajuste es menor. En este caso las métricas de test son mejores que las de test, lo que significa que el modelo generaliza muy bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a5f56-0060-4f47-a436-ee81607f0c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae502595-50b5-4652-acd8-2e6b6de2d642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
