<div align="center">

# "HOUSE PRICES: ADVANCED REGRESSION TECHNIQUES"  
## Un Enfoque H칤brido con Aprendizaje Supervisado y No Supervisado  
</div>

<br>
<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/0.png">
  
</div>

<br>


## Tecnolog칤as usadas

**Lenguajes:**
![Python](https://img.shields.io/badge/-Python-3776AB?style=flat&logo=python&logoColor=white)

**Librer칤as:**  
![NumPy](https://img.shields.io/badge/-NumPy-013243?style=flat&logo=numpy&logoColor=white) 
![Pandas](https://img.shields.io/badge/-Pandas-150458?style=flat&logo=pandas&logoColor=white) 
![Seaborn](https://img.shields.io/badge/-Seaborn-0095A7?style=flat&logo=plotly&logoColor=white) 
![Matplotlib](https://img.shields.io/badge/-Matplotlib-11557C?style=flat&logo=plotly&logoColor=white) 
![TensorFlow](https://img.shields.io/badge/-TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white) 
![Scikit-Learn](https://img.shields.io/badge/-Scikit--Learn-F7931E?style=flat&logo=scikitlearn&logoColor=white)  

## Introducci칩n

Este proyecto utiliza el dataset **"House Prices: Advanced Regression Techniques"** para predecir el valor de las viviendas mediante t칠cnicas avanzadas de **machine learning**.  

### El proceso incluye:  

- **Limpieza y transformaci칩n de los datos** para mejorar su calidad.  
- **Evaluaci칩n de m칰ltiples modelos predictivos** para seleccionar el m치s preciso.  
- **Aprendizaje no supervisado** para agrupar las casas en distintos clusters seg칰n sus caracter칤sticas.  
- **Reentrenamiento supervisado** dentro de cada cluster, mejorando el rendimiento del modelo y obteniendo informaci칩n valiosa sobre la importancia de las caracter칤sticas en cada grupo.  

Este **enfoque h칤brido** permite no solo **mejorar la precisi칩n** de las predicciones, sino tambi칠n **extraer insights clave** sobre los factores que influyen en el precio de las viviendas en diferentes segmentos del mercado.

---

<br>

## An치lisis exploratorio de los datos

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/02.png">
  
</div>

El dataset est치 compuesto por **80 variables** (incluida la variable objetivo) de naturaleza tanto **categ칩rica como num칠rica**.  

Se observa un alto porcentaje de datos faltantes en algunas columnas. Dado que el dataset tiene 1460 filas, ciertas variables presentan hasta un **90 % de valores NaN**.  

Sin embargo, no se descartar치n estas variables de inmediato, ya que a칰n podr칤an ofrecer informaci칩n 칰til. Por ejemplo, los valores nulos en la columna **`Fence`** podr칤an interpretarse como la **ausencia de cercas** y ser sustituidos por un **0**.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/03.png">
  
</div>

Lo primero que llama la atenci칩n es que algunas columnas **num칠ricas** podr칤an ser, en realidad, **categ칩ricas**. Tras revisar la descripci칩n de los datos proporcionada, podemos concluir que **MSSubClass** es, de hecho, una columna categ칩rica. Tambi칠n se observa que algunas variables presentan un **sesgo a la derecha** en su distribuci칩n.  

Algunas variables categ칩ricas tienen una **칰nica categor칤a**, lo que las hace completamente in칰tiles, por lo que deben ser **eliminadas**. Adem치s, hay otras variables en las que una **categor칤a domina significativamente** sobre las dem치s, lo que las hace susceptibles de alg칰n tipo de transformaci칩n para reducir la dimensionalidad. Por 칰ltimo, un tercer grupo de variables presenta **demasiadas categor칤as**, que en el futuro muy posiblemente deber치n ser reducidas para controlar la dimensionalidad del modelo.  

Parece que muchas variables tienen **valores at칤picos**. Sin embargo, dada la naturaleza del dataset, no se puede saber a priori si son valores **leg칤timamente altos** o si se deben a **errores en la recolecci칩n de datos**.  

Este hecho, junto con el uso de **modelos robustos** que manejan bien los outliers para la predicci칩n, descarta cualquier tipo de **eliminaci칩n generalizada** de estos valores. En su lugar, se tratar치 **cada caso de forma individual**. 

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/04.png">
  
</div>

Parece que muchas variables tienen **valores at칤picos**. Sin embargo, dada la naturaleza del dataset, no se puede saber a priori si son valores **leg칤timamente altos** o si se deben a **errores en la recolecci칩n de datos**.  

Este hecho, junto con el uso de **modelos robustos** que manejan bien los outliers para la predicci칩n, descarta cualquier tipo de **eliminaci칩n generalizada** de estos valores. En su lugar, se tratar치 **cada caso de forma individual**.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/05.png">
  
</div>

Se observan **correlaciones** tanto con **SalePrice** como entre otras variables. Por ejemplo, las correlaciones de **YearBuilt** con otras variables sugieren distintas preferencias en el dise침o de las casas a lo largo de los a침os.  

Analizando concretamente las **correlaciones de las variables num칠ricas** con la variable objetivo, vuelven a aparecer **fuertes correlaciones**, mientras que en otras variables no tanto. Sin embargo, estas 칰ltimas pueden ser **importantes para el modelo** al establecer **relaciones no lineales**.

---
<br>

## Tratamiento de los datos
A continuaci칩n se detallar치 el tratamiento de los datos de algunas de las variables a modo de ejemplo. Se puede consultar el tratamiento para cada variable en el notebook: [Tratamiento de Variables - Notebook](https://github.com/OscarDomPer/houses/blob/main/02_NTT_trat_train.ipynb)

<br>

### LotFrontage: 

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/06.png">
  
</div>

Esta variable representa la medida del frente del lote que da a la calle. Es un buen ejemplo porque ilustra la pol칤tica seguida tanto para los **outliers** como para los **NaN**.

### Outliers:  
El m치ximo de **LotFrontage** son dos propiedades de **313 pies** (aproximadamente 90 metros), lo que parece inveros칤mil para una propiedad residencial. Adem치s, de acuerdo con sus correspondientes valores de **LotArea**, la propiedad tendr칤a una forma extremadamente alargada, por lo que se asume que es un error de recolecci칩n de datos y se elimina. Este es uno de los pocos casos en los que se elimina el **outlier**. Otros casos, como una piscina de un 치rea considerablemente m치s grande que una ol칤mpica, se mantienen, ya que, si bien son at칤picos, entran dentro de lo plausible. Se conf칤a en la capacidad de los modelos m치s robustos para integrarlos en la predicci칩n.

### Valores Nulos:  
Hay **259 valores nulos**. Teniendo en cuenta que en el dataset no hay valores menores de **20 pies**, no ser칤a descabellado pensar que los **NaN** se correspondan en realidad a valores de **0** o cercanos a 0. Apoya esta teor칤a la mayor proporci칩n de terrenos de forma irregular entre las filas que tienen valores de **LotFrontage NaN**. Por lo tanto, se sustituyen los **NaN** por **0**. En la pr치ctica, se le ha aplicado este tratamiento a los valores nulos del dataset, porque su presencia parec칤a responder a la ausencia de la variable m치s que a la ausencia de su registro.

---
<br>

### Neighborhood: 

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/07.png">
  
</div>

Es una variable a priori muy interesante, sin embargo, tiene demasiadas **categor칤as**. Al ser imposible codificarlas de forma ordinal, un **onehot encoding** aumentar칤a demasiado la dimensionalidad. La soluci칩n es **reagrupar los barrios** en tres categor칤as seg칰n su **precio medio**.

Las nuevas categor칤as son susceptibles de **codificarse de forma ordinal**, lo que contribuye al objetivo de **reducir la dimensionalidad**.

---
<br>

### BsmtFinType1 y BsmtFinSF1:

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/08.png">
  
</div>

1. **BsmtFinType1 y BsmtFinSF1**: BsmtFinType1 describe el tipo de acabado principal en el s칩tano (**GLQ, ALQ, BLQ, Rec, LwQ, Unf, NA**). BsmtFinSF1 representa los pies cuadrados acabados correspondientes al tipo principal de acabado del s칩tano descrito por BsmtFinType1. Es decir, BsmtFinType1 indica la calidad del acabado en una parte del s칩tano, y BsmtFinSF1 cuantifica cu치ntos pies cuadrados corresponden a ese acabado.

2. **BsmtFinType2 y BsmtFinSF2**: BsmtFinType2 describe el segundo tipo de acabado en el s칩tano (si hay m치s de un tipo de acabado). BsmtFinSF2 indica los pies cuadrados acabados correspondientes a ese segundo tipo de acabado. Si un s칩tano tiene 치reas con diferentes tipos de acabado, BsmtFinType2 y BsmtFinSF2 complementan a BsmtFinType1 y BsmtFinSF1. Si no hay dos tipos de acabado, ambos pueden ser NA o 0.

3. **BsmtUnfSF**: Representa los pies cuadrados no acabados del s칩tano. Es decir, esta variable indica qu칠 parte del s칩tano no tiene ning칰n tipo de acabado.

4. **TotalBsmtSF**: Esta variable representa el 치rea total del s칩tano, y es la suma de todas las 치reas, tanto las acabadas (**BsmtFinSF1 + BsmtFinSF2**) como las no acabadas (**BsmtUnfSF**).

**Estas seis variables deber칤an poder resumirse en una sola**; en primer lugar, como hasta ahora le daremos un valor de 0 a los NA, luego le asignaremos valores ordinales a BsmtFintype 1 y 2, luego se pondera el valor de las 치reas correspondientes, con una f칩rmula que ser치 el valor normalizado de la correspondiente 치rea con el valor de BsmtFintype para luego sumar todas las 치reas.

---

<br>

## Selecci칩n del modelo

En primera instancia se prueban los **modelos m치s comunes**, y tambi칠n un **Stacking** y una **Red Neuronal**. Se usan dos m칠tricas: **RMSLE**, que es la que se usar치 como referencia en el reto, y **洧녠**, que se medir치n tanto en **train** como en **test**, con el objetivo de valorar el **sobreajuste**.  

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/10.png">
  
</div>

En los **modelos m치s comunes** se observan m칠tricas prometedoras en los modelos m치s **robustos** (**Gradient Boosting** y **Random Forest**). Los modelos que no funcionan bien con relaciones **no lineales** tienen un desempe침o muy pobre. Todo ello sugiere que las variables se relacionan entre s칤 de **forma compleja**.

En la familia de los **ensambladores**, solo **CatBoost** mejora a **Gradient**, pero dado que el **sobreajuste** es mayor y que el dataset es distinto a los dem치s, se descarta. A pesar de que las m칠tricas de **XGBoost** son peores, se mantendr치 este modelo, debido a que suele funcionar bien en este tipo de pruebas.

El **Stacking** no ofrece mejores resultados.

La **Red Neuronal** muestra resultados interesantes, ya que el **sobreajuste** es muy bajo, insinuando que es capaz de **generalizar**.

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/12.png">
  
</div>

Los resultados son en general **razonables**. Parece que los dos modelos que **sobreajustaban en exceso** (**GBT** y **XGB**) tienen un rendimiento algo superior a los que **generalizaron mejor** (**Stacking** y **Red Neuronal**).  

De esto se puede concluir que los **datos del conjunto de prueba** son **muy similares** a los del de entrenamiento. Seguramente, la **Red Neuronal** mejorar칤a con **datos menos homog칠neos**.

---

<br>

## Aprendizaje no supervisado.


<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/13.png">
  
</div>

---

<br>

La idea aqu칤 es, usando **aprendizaje no supervisado**, dividir el **conjunto de entrenamiento** en **clusters** para **reentrenar** en cada uno de ellos los modelos que dieron mejores resultados.  

Posteriormente, utilizando **labeling**, se divide el **conjunto de prueba** en los mismos **clusters** y se realizan las predicciones con su correspondiente modelo.  

El objetivo es doble: por un lado, **mejorar las predicciones** encontrando patrones espec칤ficos en cada cluster y, por otro, **extraer insights clave** sobre los factores que influyen en el **precio de las viviendas** en diferentes segmentos del mercado.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/14.png">
  
</div>

Dada la **gran dimensionalidad** del modelo, se valora la **inercia** de cada n칰mero de **clusters** en el **conjunto completo** de los datos y luego en **subconjuntos** con las **50, 30 y 10 variables m치s importantes** para el modelo **XGBoost**. En todos los casos, el n칰mero de **K** parece ser **2**, aunque no es una decisi칩n clara.  

Se vuelven a usar los **4 conjuntos** para obtener los **clusters** usando **KMeans=2**. Como era de esperar, al usar **todas las variables**, la **clusterizaci칩n** no es clara, pero con las **50 mejores** ya se observan **dos clusters diferenciados**.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/15.png">
  
</div>

Se selecciona **K=2** en las **50 mejores caracter칤sticas**, pues con esta combinaci칩n obtenemos **dos clusters plenamente diferenciados** y no perdemos demasiadas variables.  

Como se observa en estas gr치ficas, los **clusters** se agrupan claramente en funci칩n del **valor de las viviendas**. Esto se refleja tanto en la variable **SalePrice** (que no fue utilizada para la agrupaci칩n) como en otras variables relacionadas con **calidad y superficie**, que presentan valores m치s altos en el **cl칰ster 1**.  

Por otro lado, la variable **Age**, que indica la **antig칲edad de la vivienda**, es, como era de esperar, mayor en el **cl칰ster 0**.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/16.png">
  
</div>

Los **clusters** tienen el mismo comportamiento respecto a las **variables clave** en el **conjunto de prueba**.  

Los datos de **Gradient Boosting** mejoran a los del modelo entrenado con el **conjunto completo**, lo que sugiere que cada grupo tiene **patrones distintos**, es decir, la relaci칩n entre las **caracter칤sticas** y el **precio** es diferente entre estos subgrupos. Esta teor칤a se confirma al observar las **gr치ficas de importancia de caracter칤sticas** de cada modelo.  

En conclusi칩n, se consigue en un solo modelo capturar todas las **interacciones posibles**.  

En el caso de la **red neuronal**, los resultados **no mejoran**, seguramente debido al **tama침o limitado** del conjunto de datos original. Al partir de un dataset de **menos de 1500 muestras** y dividirlo en **dos clusters**, el n칰mero de muestras disponible para cada **submodelo de red neuronal** no fue suficiente para un entrenamiento efectivo.  

Las **redes neuronales** requieren un **mayor volumen de datos** para aprender **patrones complejos** de manera generalizable, mientras que los modelos como el **GBT** suelen ser m치s robustos en situaciones de **datos peque침os o medianos**.  

La menor tendencia a **sobreactuarse** de la red frente a **GBT** y la mayor capacidad de **generalizaci칩n** de las redes neuronales en general podr칤an hacer que, con un **n칰mero mayor de muestras de entrenamiento**, funcionen mejor en un **caso real**.  

---

  <br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/19.png">
  
</div>

Para cada **cluster** var칤a el **orden de las importancias**, indicando **patrones distintos** a la hora de predecir los **precios**.  

Las diferencias en el **orden de las importancias** se explican en gran parte en relaci칩n con el **valor de la vivienda**, estando el **cluster 0** compuesto por **viviendas de valor medio-bajo** y el **cluster 1** por **viviendas de alto valor**.  

Ciertas caracter칤sticas, como **GrLivArea** (치rea total habitable a nivel del suelo), **BsmtQualityScore** (una medida que combina la superficie y la calidad de los acabados del s칩tano) y **1stFlrSF** (superficie del primer piso), tienen gran **importancia** en ambos **cl칰steres**.  

De manera general, factores como una **mayor superficie habitable** o un **buen s칩tano** incrementan el **valor de la propiedad**.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/20.png">
  
</div>

Caracter칤sticas que en **viviendas de gran valor** se dan por sentadas, pueden representar un **factor diferencial** en **viviendas m치s modestas**.  

Por ejemplo, una **buena chimenea** (**FirePlaceQual**) puede ser determinante en una **vivienda media**. Otro buen ejemplo es el **aire acondicionado** (**CentralAir**) o una **buena calefacci칩n** (**HeatingQC**).  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/21.png">
  
</div>

Las viviendas m치s caras, por su parte, presentan una serie de **variables exclusivas asociadas con el lujo**: *PoolArea*, terraza de madera (*WoodDeckSF*) o darle importancia a la calidad de los materiales exteriores (*ExterQual*).  

Un caso curioso es **GarageArea**, que es la tercera m치s importante en el *cluster* 1. Una posible explicaci칩n ser칤a que las personas que pueden permitirse esas casas tienen varios veh칤culos y necesitan garajes m치s amplios.

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/22.png">
  
</div>

Al observar la posici칩n de **OverallQual** (Calidad general de la construcci칩n) y **OverallCond** (Condici칩n-estado general de la construcci칩n) en cada **cl칰ster**, se observa que sus posiciones est치n **invertidas**. Y no s칩lo eso, este patr칩n tambi칠n se repite en los otros dos pares de **Calidad-Condici칩n** (**ExterQual**, **ExterCond**, **BsmtQual** y **BsmtCond**).  

La explicaci칩n podr칤a ser que en **viviendas de alto valor**, la **calidad de los materiales** es m치s importante que el **estado de conservaci칩n** de los mismos. Por ejemplo, unas **escaleras de m치rmol de Carrara** incrementar치n el valor de la propiedad aunque est칠n algo gastadas. Mientras que en las **casas m치s baratas**, hechas con materiales m치s modestos, lo que prima es el **buen estado de los mismos**, ya que esto garantiza su **funcionalidad**.  

La **funcionalidad**, por cierto, tambi칠n es una variable (**Functional**) que solo aparece en el **top 30** en **Low Price Houses**.  

---

<br>

<div align="center">

  <img src="https://github.com/OscarDomPer/houses/blob/main/imaxes/23.png">
  
</div>

La variable **Neighborhood**, que se refiere al barrio en el que est치n ubicadas las propiedades, presenta gran **importancia** en ambos **cl칰steres**. Saber en qu칠 **barrios** ha ido incrementando el **precio de las viviendas** y en cu치les est치 bajando podr칤a ser una valiosa informaci칩n para predecir no solo el **valor actual** de una propiedad, sino tambi칠n su **evoluci칩n en el futuro pr칩ximo**.  

De acuerdo con los datos, barrios como **Crawford**, **Mitchell** o **Briardale** ser칤an excelentes opciones para **invertir**, mientras que ser칤a recomendable mantenerse alejado de **Meadow Village** o **Bloomington Heights**.  

